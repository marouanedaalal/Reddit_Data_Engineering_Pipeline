

# ğŸš€ Data Pipeline with Reddit, Airflow, Celery, Postgres, S3, AWS Glue, Athena, and Redshift

This project provides a comprehensive **data pipeline** solution to extract, transform, and load (ETL) Reddit data into a Redshift data warehouse. The pipeline leverages a combination of tools and services including **Apache Airflow**, **Celery**, **PostgreSQL**, **Amazon S3**, **AWS Glue**, **Amazon Athena**, and **Amazon Redshift**.

---

## ğŸ“Œ Overview

The pipeline is designed to:

1. ğŸ“ **Extract** data from Reddit using its API.
2. ğŸ—ƒï¸ **Store** the raw data into an S3 bucket from Airflow.
3. ğŸ”„ **Transform** the data using AWS Glue and Amazon Athena.
4. ğŸ“Š **Load** the transformed data into Amazon Redshift for analytics and querying.

---

## ğŸ§© Architecture

![RedditDataEngineering.png](assets%2FRedditDataEngineering.png)

1. **Reddit API**: Source of the data.
2. **Apache Airflow & Celery**: Orchestrates the ETL process and manages task distribution.
3. **PostgreSQL**: Temporary storage and metadata management.
4. **Amazon S3**: Raw data storage.
5. **AWS Glue**: Data cataloging and ETL jobs.
6. **Amazon Athena**: SQL-based data transformation.
7. **Amazon Redshift**: Data warehousing and analytics.

---

## âš™ï¸ Prerequisites

Before you begin, make sure you have:

- âœ… AWS Account with appropriate permissions for S3, Glue, Athena, and Redshift.
- âœ… Reddit API credentials.
- âœ… Docker Installation.
- âœ… Python 3.9 or higher.

---

## ğŸ“‚ Project Structure

```bash
.
â”œâ”€â”€ dags/
â”‚   â”œâ”€â”€ etl_reddit_dag.py         

â”‚   â””â”€â”€ etl_global_dag.py         

â”œâ”€â”€ etls/
â”‚   â”œâ”€â”€ reddit_etl.py             

â”‚   â””â”€â”€ global_etl.py             

â”œâ”€â”€ pipelines/
â”‚   â”œâ”€â”€ reddit_pipeline.py         

â”‚   â”œâ”€â”€ global_pipeline.py         

â”‚   â””â”€â”€ aws_s3_pipeline.py        

â”œâ”€â”€ utils/
â”‚   â””â”€â”€ constants.py              

â”œâ”€â”€ assets/
â”‚   â””â”€â”€ RedditDataEngineering.png   

â”œâ”€â”€ reddit_pipeline_architecture.png  

â””â”€â”€ README.md
```

---

## ğŸ”§ How It Works

### 1. **Extract from Reddit API**

We use `praw` to extract top posts from subreddits:

```python
reddit = praw.Reddit(...)
top_posts = reddit.subreddit("dataengineering").top(limit=50)
```

### 2. **Transform with Pandas**

Clean and format the data:

```python
df["created_utc"] = pd.to_datetime(df["created_utc"], unit="s")
```

### 3. **Upload to Amazon S3**

Upload data to S3 either directly or after saving it locally:

```python
# Save locally (method 1)
df.to_csv("reddit.csv")

# Direct upload (method 2)
s3.put_object(Bucket=bucket, Key="reddit.csv", Body=csv_buffer.getvalue())
```




---

## ğŸ³ Docker & Celery Setup

To run the full-stack version using Docker + Celery:

1. Ensure Docker & Docker Compose are installed.
2. Clone this repository and run the following command:

```bash
docker-compose up --build
```

This will start the Airflow UI at [http://localhost:8080](http://localhost:8080).

---

## ğŸ“Š Sample Output (Stored in S3)

Once the pipeline is executed, you will find the raw Reddit data in your S3 bucket:

```
reddit-data-bucket/
â”‚
â”œâ”€â”€ raw/
â”‚   â””â”€â”€ reddit_20250421_183000.csv
â”‚
â””â”€â”€ redditfolder/
    â””â”€â”€ reddit_20250421_183002.csv
```

---

## ğŸ“ Requirements

- âœ… AWS Account (S3, Glue, Athena, Redshift permissions)
- âœ… Reddit API credentials (obtain from [Reddit Developer](https://www.reddit.com/prefs/apps))
- âœ… Python 3.9+
- âœ… Docker (for full-stack setup)
- âœ… IAM roles with permissions for Glue, S3, Redshift

---

## ğŸ”‘ Environment Configuration

Update the `utils/constants.py` file with your credentials:

```python
CLIENT_ID = "your_reddit_id"
SECRET = "your_reddit_secret"
AWS_ACCESS_KEY_ID = "your_aws_key"
AWS_SECRET_ACCESS_KEY = "your_aws_secret"
AWS_BUCKET_NAME = "your_bucket"
```



